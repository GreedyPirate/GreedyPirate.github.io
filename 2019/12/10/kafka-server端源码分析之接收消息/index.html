<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="kafka server端源码分析之接收消息"><meta name="keywords" content="中间件,kafka,消息"><meta name="author" content="紫夜,undefined"><meta name="copyright" content="紫夜"><title>kafka server端源码分析之接收消息 | 紫夜の博客</title><link rel="shortcut icon" href="/my-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/gh/upupming/gitalk@36368e5dffd049e956cdbbd751ff96c28d8255cf/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6ba54465c1ff0c31b169e7a89d3dbe37";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前文"><span class="toc-number">1.</span> <span class="toc-text">前文</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#图文解析"><span class="toc-number">2.</span> <span class="toc-text">图文解析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#源码"><span class="toc-number">3.</span> <span class="toc-text">源码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生产者请求处理方法"><span class="toc-number">3.1.</span> <span class="toc-text">生产者请求处理方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ReplicaManager"><span class="toc-number">4.</span> <span class="toc-text">ReplicaManager</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#appendToLocalLog方法实现"><span class="toc-number">4.0.1.</span> <span class="toc-text">appendToLocalLog方法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Partition-appendRecordsToLeader方法实现"><span class="toc-number">4.0.2.</span> <span class="toc-text">Partition#appendRecordsToLeader方法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#appendAsLeader-gt-append实现"><span class="toc-number">4.0.3.</span> <span class="toc-text">appendAsLeader-&gt;append实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Roll-Segment-滚动日志段"><span class="toc-number">4.0.4.</span> <span class="toc-text">Roll Segment(滚动日志段)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FileChannel写入"><span class="toc-number">4.0.5.</span> <span class="toc-text">FileChannel写入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ae01.alicdn.com/kf/He0f82cbc452e4e99b7da670575752df0l.png"></div><div class="author-info__name text-center">紫夜</div><div class="author-info__description text-center">stay hungry, stay foolish</div><div class="follow-button"><a href="https://github.com/GreedyPirate" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">54</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">12</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://blog.csdn.net/yj7758423" target="_blank">我的CSDN</a><a class="author-info-links__name text-center" href="https://segmentfault.com/blog/code-craft" target="_blank">膜拜大神</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://ae01.alicdn.com/kf/H6e9ae455bca04f4098243e3f73a85c4fb.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">紫夜の博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">kafka server端源码分析之接收消息</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-10</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Kafka-Tutorial/">Kafka Tutorial</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3,378</span><span class="post-meta__separator">|</span><span>Reading time: 15 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>承接上篇搭建kafka源码环境之后，本文正式开始分析</p>
</blockquote>
<h1 id="前文"><a href="#前文" class="headerlink" title="前文"></a>前文</h1><p>在前文<a href="https://greedypirate.github.io/2019/12/06/kafka%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/">kafka网络请求处理模型</a>中提到, KafkaServer#startup方法涵盖了kafka server所有模块的初始化<br>KafkaRequestHandlerPool线程池中的KafkaRequestHandler对象通过调用KafkaApis的handle方法，处理各类网络请求</p>
<h1 id="图文解析"><a href="#图文解析" class="headerlink" title="图文解析"></a>图文解析</h1><p><img src="https://ae01.alicdn.com/kf/H46b57c5ef8eb44fbb533c5d808b49906v.png" alt="append消息流程"></p>
<p>上图是kafka server追加消息到日志的整个流程，主要分为以下几步</p>
<ol>
<li>handleProduceRequest首先过滤认证失败和leader未知的分区，定义响应回调。如果ack=0直接响应，否则继续ReplicaManager处理</li>
<li>将生产者的相关参数，如超时时间，ack，以及第1步的响应回调函数传给ReplicaManager#appendRecords，appendRecords继续调用appendToLocalLog，完成后如果ack=-1时，第一次尝试结束请求</li>
<li>appendToLocalLog则遍历所有分区，获取该分区的本地leader副本Partition对象，调用它的appendRecordsToLeader方法，为每个分区追加消息</li>
<li>Partition#appendRecordsToLeader方法中，在校验完minIsr参数后，调用Log对象appendAsLeader-&gt;append方法，里面首先计算要追加的位移，消息CRC校验，截断无效消息等</li>
<li>Log#append方法之后会判断当前activeSegment是否需要roll(新建一个)，然后调用LogSegment#append-&gt;…-&gt;FileChannel#write将消息写入日志中</li>
<li>层层返回，调用响应回调函数中的sendResponse，和<a href="https://greedypirate.github.io/2019/12/06/kafka%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/">kafka网络请求处理模型</a>一文承上启下，将Response对象放入Processor中的responseQueue，等待Processor轮询处理</li>
</ol>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p>注: TopicPartition, 包含topic和partition的值，简称TP</p>
<h2 id="生产者请求处理方法"><a href="#生产者请求处理方法" class="headerlink" title="生产者请求处理方法"></a>生产者请求处理方法</h2><p>关于KafkaRequestHandler线程类的相关源码省略，查看其run方法即可</p>
<p>KafkaApis#handle方法根据不同类型的请求，调用不同的handleXxx方法，生产者请求在handleProduceRequest方法中</p>
<p>该方法除了调用ReplicaManager#appendRecords,还对日志权限，事务，限流等做了处理，并且定义好了响应回调函数，一并作为参数传给了ReplicaManager#appendRecords方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 省略部分代码</span></span><br><span class="line"><span class="function">def <span class="title">handleProduceRequest</span><span class="params">(request: RequestChannel.Request)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 转换为具体的请求对象</span></span><br><span class="line">    val produceRequest = request.body[ProduceRequest]</span><br><span class="line">    val numBytesAppended = request.header.toStruct.sizeOf + request.sizeOfBodyInBytes</span><br><span class="line"></span><br><span class="line">    val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()</span><br><span class="line">    val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()</span><br><span class="line">    val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ((topicPartition, memoryRecords) &lt;- produceRequest.partitionRecordsOrFail.asScala) &#123;</span><br><span class="line">      <span class="comment">// 是否认证通过，是否有write权限</span></span><br><span class="line">      <span class="keyword">if</span> (!authorize(request.session, Write, Resource(Topic, topicPartition.topic, LITERAL)))</span><br><span class="line">        <span class="comment">// 忘了语法... +=是想集合添加元素，但是 -&gt;呢？ 这是map的key-&gt;value 语法</span></span><br><span class="line">        unauthorizedTopicResponses += topicPartition -&gt; <span class="keyword">new</span> PartitionResponse(Errors.TOPIC_AUTHORIZATION_FAILED)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (!metadataCache.contains(topicPartition))</span><br><span class="line">        <span class="comment">// 元数据缓存中是否有该tp，元数据缓存是由controller直接更新的</span></span><br><span class="line">        nonExistingTopicResponses += topicPartition -&gt; <span class="keyword">new</span> PartitionResponse(Errors.UNKNOWN_TOPIC_OR_PARTITION)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="comment">// 剩下的都是可用的消息</span></span><br><span class="line">        authorizedRequestInfo += (topicPartition -&gt; memoryRecords)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the callback for sending a produce response</span></span><br><span class="line">    <span class="comment">// 嵌套方法，定义响应回调，可以先不看</span></span><br><span class="line">    <span class="function">def <span class="title">sendResponseCallback</span><span class="params">(responseStatus: Map[TopicPartition, PartitionResponse])</span> </span>&#123;</span><br><span class="line">      <span class="comment">// ++表示集合合并</span></span><br><span class="line">      val mergedResponseStatus = responseStatus ++ unauthorizedTopicResponses ++ nonExistingTopicResponses</span><br><span class="line">      var errorInResponse = <span class="keyword">false</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 先打个日志，不管</span></span><br><span class="line">      mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (status.error != Errors.NONE) &#123;</span><br><span class="line">          errorInResponse = <span class="keyword">true</span></span><br><span class="line">          debug(<span class="string">"Produce request with correlation id %d from client %s on partition %s failed due to %s"</span>.format(</span><br><span class="line">            request.header.correlationId,</span><br><span class="line">            request.header.clientId,</span><br><span class="line">            topicPartition,</span><br><span class="line">            status.error.exceptionName))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 省略配额限流相关代码</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// Send the response immediately. In case of throttling, the channel has already been muted.</span></span><br><span class="line">      <span class="comment">// ack=0表示发到broker就返回，不关心副本是否写入</span></span><br><span class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</span><br><span class="line">          sendNoOpResponseExemptThrottle(request)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// ack为-1或1的响应</span></span><br><span class="line">        sendResponse(request, Some(<span class="keyword">new</span> ProduceResponse(mergedResponseStatus.asJava, maxThrottleTimeMs)), None)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 只有__admin_client客户端才能写入内部topic，例如__consumer_offset</span></span><br><span class="line">  val internalTopicsAllowed = request.header.clientId == AdminUtils.AdminClientId</span><br><span class="line"></span><br><span class="line">  <span class="comment">// call the replica manager to append messages to the replicas</span></span><br><span class="line">  <span class="comment">// 开始调用副本管理器追加消息</span></span><br><span class="line">  replicaManager.appendRecords(</span><br><span class="line">  	<span class="comment">// 超时时间, 客户端Sender中的requestTimeoutMs，表示客户端请求超时</span></span><br><span class="line">  	timeout = produceRequest.timeout.toLong,</span><br><span class="line">  	<span class="comment">// ack参数</span></span><br><span class="line">  	requiredAcks = produceRequest.acks,</span><br><span class="line">  	<span class="comment">// 是否允许添加内部topic消息</span></span><br><span class="line">  	internalTopicsAllowed = internalTopicsAllowed,</span><br><span class="line">  	<span class="comment">// 是否来自client，也有可能来自别的broker</span></span><br><span class="line">  	isFromClient = <span class="keyword">true</span>,</span><br><span class="line">  	<span class="comment">// 消息体</span></span><br><span class="line">  	entriesPerPartition = authorizedRequestInfo,</span><br><span class="line">  	<span class="comment">// 响应函数</span></span><br><span class="line">  	responseCallback = sendResponseCallback,</span><br><span class="line">  	<span class="comment">// 状态转换函数</span></span><br><span class="line">  	recordConversionStatsCallback = processingStatsCallback</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if the request is put into the purgatory, it will have a held reference and hence cannot be garbage collected;</span></span><br><span class="line">  <span class="comment">// hence we clear its data here in order to let GC reclaim its memory since it is already appended to log</span></span><br><span class="line">  <span class="comment">// 如果需要被放入purgatory，清空引用让GC回收, 因为已经append到log了</span></span><br><span class="line">  produceRequest.clearPartitionRecords()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h1><p>ReplicaManager的主要功能是对分区副本层面做管理，包含日志写入，读取，ISR的变更，副本同步等。</p>
<p>appendRecords的方法注释如下：将消费追加到分区的leader副本，然后等待它们被follower副本复制，回调函数将会在超时或者ack条件满足是触发</p>
<p>该方法主要是在append消息之后，对当前请求的处理。ack=-1尝试完成当前请求，在ack=1时直接调用响应函数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">appendRecords</span><span class="params">(... )</span> </span>&#123; <span class="comment">//参数参考上面</span></span><br><span class="line">    <span class="comment">// 简单的校验ack合法性，-1，0，1才合法</span></span><br><span class="line">    <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123;</span><br><span class="line">      val sTime = time.milliseconds</span><br><span class="line">      <span class="comment">// 写入到本地broker中, 返回每个TPLogAppendResult =&gt; LogAppendInfo和异常</span></span><br><span class="line">      val localProduceResults = appendToLocalLog(internalTopicsAllowed = internalTopicsAllowed,</span><br><span class="line">        isFromClient = isFromClient, entriesPerPartition, requiredAcks)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// produceStatus类型:Map[TopicPartition, ProducePartitionStatus]</span></span><br><span class="line">      <span class="comment">// 这个map保存的是每个TopicPartition append后的状态，状态包括：LEO和结果，结果里面有是否append出现错误等</span></span><br><span class="line">      val produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">        topicPartition -&gt;</span><br><span class="line">                ProducePartitionStatus(</span><br><span class="line">                  result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset ， LEO </span></span><br><span class="line">                  <span class="keyword">new</span> PartitionResponse(result.error, result.info.firstOffset.getOrElse(-<span class="number">1</span>), result.info.logAppendTime, result.info.logStartOffset)) <span class="comment">// response status</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      recordConversionStatsCallback(localProduceResults.mapValues(_.info.recordConversionStats))</span><br><span class="line"></span><br><span class="line">      <span class="comment">// ack为-1时需要follower同步，需要放入延迟队列中，等待条件满足后返回</span></span><br><span class="line">      <span class="keyword">if</span> (delayedProduceRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</span><br><span class="line">        <span class="comment">// create delayed produce operation</span></span><br><span class="line">        <span class="comment">// ack和消息append后的结果</span></span><br><span class="line">        val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)</span><br><span class="line">        <span class="comment">// 注意看里面的初始化语句块</span></span><br><span class="line">        val delayedProduce = <span class="keyword">new</span> DelayedProduce(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback, delayedProduceLock)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 就是TopicPartition集合</span></span><br><span class="line">        val producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> TopicPartitionOperationKey(_)).toSeq</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一次尝试结束处理，否则丢入purgatory中，因为下一批消息可能已经到达将这批请求结束</span></span><br><span class="line">        delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 这是ack=1的时候，leader写入完了，就返回，之前已经处理过ack=0了</span></span><br><span class="line">        val produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</span><br><span class="line">        responseCallback(produceResponseStatus)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// ack参数无效后直接返回错误</span></span><br><span class="line">      val responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</span><br><span class="line">        topicPartition -&gt; <span class="keyword">new</span> PartitionResponse(Errors.INVALID_REQUIRED_ACKS,</span><br><span class="line">          LogAppendInfo.UnknownLogAppendInfo.firstOffset.getOrElse(-<span class="number">1</span>), RecordBatch.NO_TIMESTAMP, LogAppendInfo.UnknownLogAppendInfo.logStartOffset)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 最后调用传进来的响应回调方法</span></span><br><span class="line">      responseCallback(responseStatus)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="appendToLocalLog方法实现"><a href="#appendToLocalLog方法实现" class="headerlink" title="appendToLocalLog方法实现"></a>appendToLocalLog方法实现</h3><p>appendToLocalLog开始遍历分区消息集合，Map[TopicPartition, MemoryRecords]对象，<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Append the messages to the local replica logs</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> internalTopicsAllowed 是否允许操作内部topic</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> isFromClient true，来自客户端</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> entriesPerPartition 消息体</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> requiredAcks ack参数</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> Map[TopicPartition, LogAppendResult]</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">appendToLocalLog</span><span class="params">(internalTopicsAllowed: Boolean,</span></span></span><br><span class="line"><span class="function"><span class="params">                           isFromClient: Boolean,</span></span></span><br><span class="line"><span class="function"><span class="params">                           entriesPerPartition: Map[TopicPartition, MemoryRecords],</span></span></span><br><span class="line"><span class="function"><span class="params">                           requiredAcks: Short)</span>: Map[TopicPartition, LogAppendResult] </span>= &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 遍历消息集合，追加消息 map里的case表示是个匿名偏函数</span></span><br><span class="line">	entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt;</span><br><span class="line">	  <span class="comment">// 如果是内部topic，但没有内部topic的操作权限，就报错，内部topic只有两个__consumer_offsets和__transaction_state</span></span><br><span class="line">	</span><br><span class="line">	  <span class="comment">// 获取当前tp的leader Partition对象</span></span><br><span class="line">      val (partition, _) = getPartitionAndLeaderReplicaIfLocal(topicPartition)</span><br><span class="line">      val info = partition.appendRecordsToLeader(records, isFromClient, requiredAcks)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 向一个tp中追加消息结束，返回结果</span></span><br><span class="line">      (topicPartition, LogAppendResult(info))	    </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Partition-appendRecordsToLeader方法实现"><a href="#Partition-appendRecordsToLeader方法实现" class="headerlink" title="Partition#appendRecordsToLeader方法实现"></a>Partition#appendRecordsToLeader方法实现</h3><p>Partition对象的appendRecordsToLeader方法中检验ack=-1时，min.insync.replicas必须大于ISR个数<br>然后调用Log对象的appendAsLeader-&gt;append方法，追加完消息后，第二次尝试完成生产者请求</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">appendRecordsToLeader</span><span class="params">(records: MemoryRecords, isFromClient: Boolean, requiredAcks: Int = <span class="number">0</span>)</span>: LogAppendInfo </span>= &#123;</span><br><span class="line">    <span class="comment">// inReadLock是一个柯里化函数，第二个参数是一个函数，返回值是LogAppendInfo和HW是否增加的bool值</span></span><br><span class="line">    <span class="comment">// 相当于给方法加了读锁</span></span><br><span class="line">    val (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">      <span class="comment">// leaderReplicaIfLocal表示本地broker中的leader副本</span></span><br><span class="line">      leaderReplicaIfLocal match &#123;</span><br><span class="line">        <span class="comment">//如果存在的话</span></span><br><span class="line">        <span class="function"><span class="keyword">case</span> <span class="title">Some</span><span class="params">(leaderReplica)</span> </span>=&gt;</span><br><span class="line">          <span class="comment">// 获取Replica中的Log对象</span></span><br><span class="line">          val log = leaderReplica.log.get</span><br><span class="line">          <span class="comment">// min.insync.replicas参数</span></span><br><span class="line">          val minIsr = log.config.minInSyncReplicas</span><br><span class="line">          <span class="comment">// Set[Replica] ISR大小</span></span><br><span class="line">          val inSyncSize = inSyncReplicas.size</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></span><br><span class="line">          <span class="comment">// 如果isr的个数没有满足min.insync.replicas就报错，需要知道的是min.insync.replicas是和ack=-1一起使用的</span></span><br><span class="line">          <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == -<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NotEnoughReplicasException(<span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%d]"</span></span><br><span class="line">              .format(topicPartition, inSyncSize, minIsr))</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 真正的消息追加交给Log对象</span></span><br><span class="line">          val info = log.appendAsLeader(records, leaderEpoch = <span class="keyword">this</span>.leaderEpoch, isFromClient)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 检查Purgatory中，这个tp的操作是否结束，第二次尝试结束处理请求</span></span><br><span class="line">          replicaManager.tryCompleteDelayedFetch(TopicPartitionOperationKey(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</span><br><span class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></span><br><span class="line">          (info, maybeIncrementLeaderHW(leaderReplica))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// some delayed operations may be unblocked after HW changed</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented)</span><br><span class="line">      tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">    info</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="appendAsLeader-gt-append实现"><a href="#appendAsLeader-gt-append实现" class="headerlink" title="appendAsLeader-&gt;append实现"></a>appendAsLeader-&gt;append实现</h3><p>appendAsLeader方法直接调用了append方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">append</span><span class="params">(records: MemoryRecords, isFromClient: Boolean, assignOffsets: Boolean, leaderEpoch: Int)</span>: LogAppendInfo </span>= &#123;</span><br><span class="line">    maybeHandleIOException(s<span class="string">"Error while appending records to $topicPartition in dir $&#123;dir.getParent&#125;"</span>) &#123;</span><br><span class="line">      val appendInfo = analyzeAndValidateRecords(records, isFromClient = isFromClient)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// return if we have no valid messages or if this is a duplicate of the last appended entry</span></span><br><span class="line">      <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> appendInfo</span><br><span class="line"></span><br><span class="line">      <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></span><br><span class="line">      var validRecords = trimInvalidBytes(records, appendInfo)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      lock <span class="keyword">synchronized</span> &#123;</span><br><span class="line">      	<span class="comment">// assignOffsets写死为true，就不看else了</span></span><br><span class="line">        <span class="keyword">if</span> (assignOffsets) &#123;</span><br><span class="line">          <span class="comment">// assign offsets to the message set</span></span><br><span class="line">          val offset = <span class="keyword">new</span> LongRef(nextOffsetMetadata.messageOffset)</span><br><span class="line">          <span class="comment">// firstOffset又重新赋值了</span></span><br><span class="line">          appendInfo.firstOffset = Some(offset.value)</span><br><span class="line">          val now = time.milliseconds</span><br><span class="line">          <span class="comment">// 各种验证</span></span><br><span class="line">          val validateAndOffsetAssignResult = LogValidator.validateMessagesAndAssignOffsets(validRecords,</span><br><span class="line">              offset,</span><br><span class="line">              time,</span><br><span class="line">              now,</span><br><span class="line">              appendInfo.sourceCodec,</span><br><span class="line">              appendInfo.targetCodec,</span><br><span class="line">              config.compact,</span><br><span class="line">              config.messageFormatVersion.recordVersion.value,</span><br><span class="line">              config.messageTimestampType,</span><br><span class="line">              config.messageTimestampDifferenceMaxMs,</span><br><span class="line">              leaderEpoch,</span><br><span class="line">              isFromClient)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 验证通过后的消息</span></span><br><span class="line">          validRecords = validateAndOffsetAssignResult.validatedRecords</span><br><span class="line">          <span class="comment">// 根据校验结果完善appendInfo对象</span></span><br><span class="line">          appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</span><br><span class="line">          appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</span><br><span class="line">          appendInfo.lastOffset = offset.value - <span class="number">1</span></span><br><span class="line">          appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats</span><br><span class="line">          <span class="keyword">if</span> (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)</span><br><span class="line">            appendInfo.logAppendTime = now</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</span><br><span class="line">            <span class="keyword">for</span> (batch &lt;- validRecords.batches.asScala) &#123;</span><br><span class="line">              <span class="comment">// 每一批消息不能比max.message.bytes大</span></span><br><span class="line">              <span class="keyword">if</span> (batch.sizeInBytes &gt; config.maxMessageSize) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RecordTooLargeException(s<span class="string">"Message batch size is $&#123;batch.sizeInBytes&#125; bytes in append to"</span> +</span><br><span class="line">                  s<span class="string">"partition $topicPartition which exceeds the maximum configured size of $&#123;config.maxMessageSize&#125;."</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; </span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the epoch cache with the epoch stamped onto the message by the leader</span></span><br><span class="line">        validRecords.batches.asScala.foreach &#123; batch =&gt;</span><br><span class="line">          <span class="keyword">if</span> (batch.magic &gt;= RecordBatch.MAGIC_VALUE_V2)</span><br><span class="line">            _leaderEpochCache.assign(batch.partitionLeaderEpoch, batch.baseOffset)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// check messages set size may be exceed config.segmentSize</span></span><br><span class="line">        <span class="comment">// MemoryRecords总消息不能比segment.bytes大</span></span><br><span class="line">        <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> RecordBatchTooLargeException(s<span class="string">"Message batch size is $&#123;validRecords.sizeInBytes&#125; bytes in append "</span> +</span><br><span class="line">            s<span class="string">"to partition $topicPartition, which exceeds the maximum configured segment size of $&#123;config.segmentSize&#125;."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// maybe roll the log if this segment is full</span></span><br><span class="line">        <span class="comment">// 是否需要生成一个新的segment，具体判断条件见下文</span></span><br><span class="line">        val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保存位移的VO</span></span><br><span class="line">        val logOffsetMetadata = LogOffsetMetadata(</span><br><span class="line">          messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,</span><br><span class="line">          segmentBaseOffset = segment.baseOffset,</span><br><span class="line">          relativePositionInSegment = segment.size)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 真正append日志的是LogSegment对象</span></span><br><span class="line">        segment.append(largestOffset = appendInfo.lastOffset,</span><br><span class="line">          largestTimestamp = appendInfo.maxTimestamp,</span><br><span class="line">          shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</span><br><span class="line">          records = validRecords)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新LEO，lastOffset + 1</span></span><br><span class="line">        updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</span><br><span class="line">          flush()</span><br><span class="line"></span><br><span class="line">        appendInfo</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="Roll-Segment-滚动日志段"><a href="#Roll-Segment-滚动日志段" class="headerlink" title="Roll Segment(滚动日志段)"></a>Roll Segment(滚动日志段)</h3><p>首先说一下什么是Roll，可以联想一下log4j日志的滚动，就是在一定条件下，自动生成新文件，常见的log4j一个小时Roll一次，还有用过ES的同学，也可以联想一下Roll Index<br>上文的maybeRoll方法返回要追加消息的segment，要么用当前的activeSegment，要么新生成一个<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">maybeRoll</span><span class="params">(messagesSize: Int, appendInfo: LogAppendInfo)</span>: LogSegment </span>= &#123;</span><br><span class="line">    val segment = activeSegment</span><br><span class="line">    val now = time.milliseconds</span><br><span class="line"></span><br><span class="line">    val maxTimestampInMessages = appendInfo.maxTimestamp</span><br><span class="line">    val maxOffsetInMessages = appendInfo.lastOffset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (segment.shouldRoll(messagesSize, maxTimestampInMessages, maxOffsetInMessages, now)) &#123;</span><br><span class="line">      <span class="comment">// 省略了代码</span></span><br><span class="line">      roll(maxOffsetInMessages - Integer.MAX_VALUE)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 不需要Roll，就返回当前正在使用的Segment：activeSegment</span></span><br><span class="line">      segment</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Roll方法其实就是在新建.log, .index, .timeIndex文件，如果用了事务，还会有.txnindex文件<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">roll</span><span class="params">(expectedNextOffset: Long = <span class="number">0</span>)</span>: LogSegment </span>= &#123;</span><br><span class="line">    maybeHandleIOException(s<span class="string">"Error while rolling log segment for $topicPartition in dir $&#123;dir.getParent&#125;"</span>) &#123;</span><br><span class="line">      val start = time.hiResClockMs()</span><br><span class="line">      lock <span class="keyword">synchronized</span> &#123;</span><br><span class="line">        checkIfMemoryMappedBufferClosed()</span><br><span class="line">        val newOffset = math.max(expectedNextOffset, logEndOffset)</span><br><span class="line">        <span class="comment">// 新建.log, .index, .timeIndex文件，如果用了事务，还会有.txnindex文件</span></span><br><span class="line">        val logFile = Log.logFile(dir, newOffset)</span><br><span class="line">        val offsetIdxFile = offsetIndexFile(dir, newOffset)</span><br><span class="line">        val timeIdxFile = timeIndexFile(dir, newOffset)</span><br><span class="line">        val txnIdxFile = transactionIndexFile(dir, newOffset)</span><br><span class="line">        <span class="comment">// 检查是否已存在以上文件，存在则先删除</span></span><br><span class="line">        <span class="keyword">for</span> (file &lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) <span class="keyword">if</span> file.exists) &#123;</span><br><span class="line">          warn(s<span class="string">"Newly rolled segment file $&#123;file.getAbsolutePath&#125; already exists; deleting it first"</span>)</span><br><span class="line">          Files.delete(file.toPath)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// segments使用一个跳表构建的Map，说明Segment使用跳表组织的</span></span><br><span class="line">        <span class="comment">// key是Segment的baseOffset，value是Segment对象</span></span><br><span class="line">        Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建LogSegment，添加到segments集合里</span></span><br><span class="line">        val segment = LogSegment.open(dir,</span><br><span class="line">          baseOffset = newOffset,</span><br><span class="line">          config,</span><br><span class="line">          time = time,</span><br><span class="line">          fileAlreadyExists = <span class="keyword">false</span>,</span><br><span class="line">          initFileSize = initFileSize,</span><br><span class="line">          preallocate = config.preallocate)</span><br><span class="line">        val prev = addSegment(segment)</span><br><span class="line">        <span class="comment">// 说明已存在</span></span><br><span class="line">        <span class="keyword">if</span> (prev != <span class="keyword">null</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(s<span class="string">"Trying to roll a new log segment for topic partition $topicPartition with "</span> +</span><br><span class="line">            s<span class="string">"start offset $newOffset while it already exists."</span>)</span><br><span class="line">        <span class="comment">// 更新LEO</span></span><br><span class="line">        updateLogEndOffset(nextOffsetMetadata.messageOffset)</span><br><span class="line">        <span class="comment">// 将recoveryPoint到新segment offset，也就是老的segment刷盘，包含4个文件：.log, .index, .timeIndex，.txnindex</span></span><br><span class="line">        scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; flush(newOffset), delay = <span class="number">0L</span>)</span><br><span class="line"></span><br><span class="line">        segment</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="FileChannel写入"><a href="#FileChannel写入" class="headerlink" title="FileChannel写入"></a>FileChannel写入</h3><p>最后的写入过程如下<br>MemoryRecords是生产者发送MemoryRecords，也是server端接收的对象，因此把写入方法定义在了MemoryRecords中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">append</span><span class="params">(MemoryRecords records)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> written = records.writeFullyTo(channel);</span><br><span class="line">    size.getAndAdd(written);</span><br><span class="line">    <span class="keyword">return</span> written;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">writeFullyTo</span><span class="params">(GatheringByteChannel channel)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    buffer.mark();</span><br><span class="line">    <span class="keyword">int</span> written = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (written &lt; sizeInBytes())</span><br><span class="line">        written += channel.write(buffer);</span><br><span class="line">    buffer.reset();</span><br><span class="line">    <span class="keyword">return</span> written;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总的来说kafka在ReplicaManager,Partition,Log,LogSegment对象的层层调用来append消息。</p>
<p>在ack=-1时，因为follower同步之后，才算是消息提交(commit)，而在消息append过程中，并不知道什么时候follower完成同步</p>
<p>kafka的做法是多次尝试完成生产者请求，因此在源码中我们可以看到在append完成后，还会尝试完成生产者请求，否则放入Purgatory中监听(tryCompleteElseWatch)。</p>
<p>等待follower副本同步完成，再次尝试完成生产者请求(tryCompleteDelayedFetch)</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">紫夜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://greedypirate.github.io/2019/12/10/kafka-server端源码分析之接收消息/">https://greedypirate.github.io/2019/12/10/kafka-server端源码分析之接收消息/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/中间件/">中间件</a><a class="post-meta__tags" href="/tags/kafka/">kafka</a><a class="post-meta__tags" href="/tags/消息/">消息</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://ae01.alicdn.com/kf/H50b5d4d79e454447974210dae2d054435.png"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://ae01.alicdn.com/kf/H45f5b133580045879faaa5fcbe9b598fu.png"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="addthis_inline_share_toolbox pull-right"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b6532776de86c85" async></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/12/12/kafka导出消息日志及索引原理分析/"><i class="fa fa-chevron-left">  </i><span>kafka导出消息日志及索引原理分析</span></a></div><div class="next-post pull-right"><a href="/2019/12/06/kafka网络请求处理模型/"><span>kafka网络请求处理模型</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '0349a7a18b86f2a653e3',
  clientSecret: '567ba8cefbe2ecffbbc04e676652ae4b43e7f952',
  repo: 'GreedyPirate.github.io',
  owner: 'GreedyPirate',
  admin: 'GreedyPirate',
  id: md5(decodeURI(location.pathname)),
  language: ''
})
gitalk.render('gitalk-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By 紫夜</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://GreedyPirate.github.io">blog</a>!</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>